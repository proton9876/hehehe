{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Extract features and target\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Function to normalize features\n",
    "def normalize_features(X):\n",
    "    X_normalized = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "    return X_normalized\n",
    "\n",
    "# Normalize features\n",
    "X_normalized = normalize_features(X)\n",
    "\n",
    "# Add intercept term\n",
    "X_normalized = np.hstack((np.ones((X_normalized.shape[0], 1)), X_normalized))\n",
    "\n",
    "# Divide dataset into training and testing sets (70:30 ratio)\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(X_normalized))\n",
    "split_index = int(0.7 * len(X_normalized))\n",
    "X_train, X_test = X_normalized[indices[:split_index]], X_normalized[indices[split_index:]]\n",
    "y_train, y_test = y[indices[:split_index]], y[indices[split_index:]]\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Function to calculate logistic regression cost\n",
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    cost = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "    return cost\n",
    "\n",
    "# Function to perform logistic regression gradient descent\n",
    "def gradient_descent(X, y, theta, learning_rate, iterations):\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    for i in range(iterations):\n",
    "        h = sigmoid(X.dot(theta))\n",
    "        gradient = (1 / m) * X.T.dot(h - y)\n",
    "        theta -= learning_rate * gradient\n",
    "        cost_history[i] = compute_cost(X, y, theta)\n",
    "    return theta, cost_history\n",
    "\n",
    "# Initialize theta and hyperparameters\n",
    "theta = np.zeros(X_train.shape[1])\n",
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "# Perform gradient descent to train the model\n",
    "theta_trained, cost_history = gradient_descent(X_train, y_train, theta, learning_rate, iterations)\n",
    "\n",
    "# Function to make predictions\n",
    "def predict(X, theta):\n",
    "    probabilities = sigmoid(X.dot(theta))\n",
    "    return [1 if p >= 0.5 else 0 for p in probabilities]\n",
    "\n",
    "# Make predictions on training and testing sets\n",
    "y_train_pred = predict(X_train, theta_trained)\n",
    "y_test_pred = predict(X_test, theta_trained)\n",
    "\n",
    "# Function to calculate confusion matrix\n",
    "def confusion_matrix(actual, predicted):\n",
    "    tp = np.sum((actual == 1) & (predicted == 1))\n",
    "    tn = np.sum((actual == 0) & (predicted == 0))\n",
    "    fp = np.sum((actual == 0) & (predicted == 1))\n",
    "    fn = np.sum((actual == 1) & (predicted == 0))\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "# Calculate confusion matrix for training and testing sets\n",
    "tp_train, tn_train, fp_train, fn_train = confusion_matrix(y_train, y_train_pred)\n",
    "tp_test, tn_test, fp_test, fn_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Function to calculate sensitivity\n",
    "def sensitivity(tp, fn):\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "# Function to calculate specificity\n",
    "def specificity(tn, fp):\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "# Calculate sensitivity and specificity for training and testing sets\n",
    "sensitivity_train = sensitivity(tp_train, fn_train)\n",
    "specificity_train = specificity(tn_train, fp_train)\n",
    "sensitivity_test = sensitivity(tp_test, fn_test)\n",
    "specificity_test = specificity(tn_test, fp_test)\n",
    "\n",
    "# Print confusion matrix, sensitivity, and specificity\n",
    "print(\"Confusion Matrix for Training Set:\")\n",
    "print(\"TP:\", tp_train, \"TN:\", tn_train, \"FP:\", fp_train, \"FN:\", fn_train)\n",
    "print(\"Sensitivity (Training):\", sensitivity_train)\n",
    "print(\"Specificity (Training):\", specificity_train)\n",
    "print(\"\\nConfusion Matrix for Testing Set:\")\n",
    "print(\"TP:\", tp_test, \"TN:\", tn_test, \"FP:\", fp_test, \"FN:\", fn_test)\n",
    "print(\"Sensitivity (Testing):\", sensitivity_test)\n",
    "print(\"Specificity (Testing):\", specificity_test)\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def accuracy(actual, predicted):\n",
    "    return np.mean(actual == predicted)\n",
    "\n",
    "# Calculate training and testing accuracies\n",
    "training_accuracy = accuracy(y_train, y_train_pred)\n",
    "testing_accuracy = accuracy(y_test, y_test_pred)\n",
    "\n",
    "# Plotting training and testing accuracy vs iteration number\n",
    "plt.plot(range(iterations), cost_history, color='blue')\n",
    "plt.title('Training Accuracy vs Iteration Number')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training Accuracy:\", training_accuracy)\n",
    "print(\"Testing Accuracy:\", testing_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
